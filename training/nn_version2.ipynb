{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pygame installed, ignoring import\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import Features, ReplayBuffer, NetModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T18:45:41.039114Z",
     "start_time": "2023-11-30T18:45:36.309522200Z"
    }
   },
   "id": "617618c2ff26b81b"
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "# agent\n",
    "class SelectActionNet:\n",
    "    def __init__(self, feature_estimate):\n",
    "        self.feature = feature_estimate\n",
    "\n",
    "    def select_action(self):\n",
    "        features = self.feature\n",
    "\n",
    "        c_interval_length = features.expected.mean() * 0.35 + 0.05\n",
    "        interval_score = c_interval_length * features.interval_length\n",
    "\n",
    "        # c_trace = 0.15 * features.expected\n",
    "        # trace_score = features.opp_acc_trace * c_trace\n",
    "\n",
    "        rand = np.random.random(features.n_bandits) * 0.0001\n",
    "\n",
    "        step_feature = np.full(features.n_bandits, features.step/2000)\n",
    "        data_feature = np.stack((step_feature, feature.expected, feature.pro_lower_bound, feature.pro_upper_bound, feature.interval_length, feature.num_selection/25, feature.num_opp_selection/25, feature.sum_num/50, feature.num_ratio, feature.ratio_reward, feature.get_reward_th, feature.num_opp_before/10, feature.num_before/10, feature.num_opp_after/10, feature.num_after/10, feature.opp_bunching, feature.opp_rep_trace), axis=1)\n",
    "        \n",
    "        # print(data_feature.shape)\n",
    "        net_expect_value = network_model.get_value(data_feature)\n",
    "\n",
    "        # scores = net_expect_value + interval_score + trace_score + rand\n",
    "        scores = net_expect_value + interval_score + rand\n",
    "\n",
    "        # select_action = int(np.argmax(scores))\n",
    "        \n",
    "        # print(net_expect_value.shape)\n",
    "        \n",
    "        # select_action = int(np.argmax(net_expect_value))\n",
    "\n",
    "        # return select_action\n",
    "        \n",
    "        return scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:47:44.628457800Z",
     "start_time": "2023-12-01T05:47:44.299309Z"
    }
   },
   "id": "1e58dc82820f46ab"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "def agent(observation, configuration):\n",
    "    global feature, selector, total_reward\n",
    "\n",
    "    player = observation.agentIndex\n",
    "    opponent = 1 - player\n",
    "    step = observation.step\n",
    "    new_reward = observation.reward\n",
    "\n",
    "    if step == 0:\n",
    "        feature = Features(configuration.banditCount)\n",
    "        selector = SelectActionNet(feature)\n",
    "        total_reward = 0\n",
    "    else:\n",
    "        action = observation.lastActions[player]\n",
    "        opp_action = observation.lastActions[opponent]\n",
    "        reward = new_reward - total_reward\n",
    "\n",
    "        feature_value = (feature.step/2000, feature.expected[action], feature.pro_lower_bound[action], feature.pro_upper_bound[action],feature.interval_length[action], feature.num_selection[action]/25, feature.num_opp_selection[action]/25, feature.sum_num[action]/50, feature.num_ratio[action], feature.ratio_reward[action], feature.get_reward_th[action], feature.num_opp_before[action]/10, feature.num_before[action]/10, feature.num_opp_after[action]/10, feature.num_after[action]/10, feature.opp_bunching[action], feature.opp_rep_trace[action], reward)\n",
    "        replay_buffer.push(feature_value)\n",
    "\n",
    "        feature.feature_update(step, action, reward, opp_action)\n",
    "        total_reward = new_reward\n",
    "\n",
    "    action = selector.select_action()\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T04:16:16.494294500Z",
     "start_time": "2023-12-01T04:16:16.436813900Z"
    }
   },
   "id": "df3a29e4486f0986"
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "def agent(observation, configuration):\n",
    "    global feature, selector, total_reward, policy_params\n",
    "\n",
    "    player = observation.agentIndex\n",
    "    opponent = 1 - player\n",
    "    step = observation.step\n",
    "    new_reward = observation.reward\n",
    "\n",
    "    if step == 0:\n",
    "        feature = Features(configuration.banditCount)\n",
    "        selector = SelectActionNet(feature)\n",
    "        total_reward = 0\n",
    "        policy_params = np.zeros(100)\n",
    "    else:\n",
    "        action = observation.lastActions[player]\n",
    "        opp_action = observation.lastActions[opponent]\n",
    "        reward = new_reward - total_reward\n",
    "\n",
    "        feature_value = (feature.step/2000, feature.expected[action], feature.pro_lower_bound[action], feature.pro_upper_bound[action],feature.interval_length[action], feature.num_selection[action]/25, feature.num_opp_selection[action]/25, feature.sum_num[action]/50, feature.num_ratio[action], feature.ratio_reward[action], feature.get_reward_th[action], feature.num_opp_before[action]/10, feature.num_before[action]/10, feature.num_opp_after[action]/10, feature.num_after[action]/10, feature.opp_bunching[action], feature.opp_rep_trace[action], reward)\n",
    "        replay_buffer.push(feature_value)\n",
    "\n",
    "        feature.feature_update(step, action, reward, opp_action)\n",
    "        total_reward = new_reward\n",
    "\n",
    "        score = selector.select_action().reshape(100,)\n",
    "\n",
    "        policy_params = 0.1 * (score - np.mean(score)) / np.std(score) + 0.9 * policy_params\n",
    "\n",
    "    action_probs = np.exp(policy_params) / np.sum(np.exp(policy_params))\n",
    "\n",
    "    # action = np.random.choice(100, p=action_probs)\n",
    "    if observation.step > 100:\n",
    "        action = int(np.argmax(action_probs))\n",
    "    else:\n",
    "        action = np.random.choice(100, p=action_probs)\n",
    "    \n",
    "    # action = selector.select_action()\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T06:29:00.171350700Z",
     "start_time": "2023-12-01T06:29:00.156354900Z"
    }
   },
   "id": "95ad4fe775a8d8a1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def agent_opp(observation, configuration):\n",
    "    global feature_opp, selector_opp, total_reward_opp\n",
    "\n",
    "    player = observation.agentIndex\n",
    "    opponent = 1 - player\n",
    "    step = observation.step\n",
    "    new_reward = observation.reward\n",
    "\n",
    "    if step == 0:\n",
    "        feature_opp = Features(configuration.banditCount)\n",
    "        selector_opp = SelectActionNet(feature_opp)\n",
    "        total_reward_opp = 0\n",
    "    else:\n",
    "        action = observation.lastActions[player]\n",
    "        opp_action = observation.lastActions[opponent]\n",
    "        reward = new_reward - total_reward_opp\n",
    "\n",
    "        feature_value = (feature_opp.step/2000, feature_opp.expected[action], feature_opp.pro_lower_bound[action], feature_opp.pro_upper_bound[action],feature_opp.interval_length[action], feature_opp.num_selection[action]/25, feature_opp.num_opp_selection[action]/25, feature_opp.sum_num[action]/50, feature_opp.num_ratio[action], feature_opp.ratio_reward[action], feature_opp.get_reward_th[action], feature_opp.num_opp_before[action]/10, feature_opp.num_before[action]/10, feature_opp.num_opp_after[action]/10, feature_opp.num_after[action]/10, feature_opp.opp_bunching[action], feature_opp.opp_rep_trace[action], reward)\n",
    "\n",
    "        replay_buffer.push(feature_value)\n",
    "\n",
    "        feature_opp.feature_update(step, action, reward, opp_action)\n",
    "        total_reward_opp = new_reward\n",
    "\n",
    "    action = selector_opp.select_action()\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T18:45:55.533550600Z",
     "start_time": "2023-11-30T18:45:55.407993800Z"
    }
   },
   "id": "d5c60d15afbd8ab9"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "class ClasssfyNN(nn.Module):\n",
    "    \"\"\" 定义神经网络 \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1=16, hidden_size2=16):\n",
    "        super(ClasssfyNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        # out = self.relu1(out)\n",
    "        out = nn.functional.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.relu2(out)\n",
    "        out = nn.functional.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "    \n",
    "        # out = nn.functional.softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\" 定义经验回放缓冲区 \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=4000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            self.buffer[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def refresh(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "\n",
    "class NetModel:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.network = ClasssfyNN(input_size)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        \"\"\" 输入x应为矩阵形式 \"\"\"\n",
    "        inputs = torch.from_numpy(x.astype(np.float32))\n",
    "        outputs = self.network(inputs)\n",
    "        y = outputs.detach().numpy().T\n",
    "        return y\n",
    "\n",
    "    def train(self, data, lr=0.01, num_epochs=100):\n",
    "        \"\"\" 输入训练数据为ReplayBuffer中的buffer列表 \"\"\"\n",
    "        # 处理训练数据\n",
    "        data_train = np.array(data)\n",
    "        x_train = torch.from_numpy(data_train[:, 0:self.input_size].astype(np.float32))\n",
    "        y_train = torch.from_numpy(data_train[:, self.input_size:(self.input_size + 1)].astype(np.float32))\n",
    "\n",
    "        # 定义损失函数和优化器\n",
    "        criterion = nn.BCELoss()\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        # optimizer = optim.SGD(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # 训练模型\n",
    "        for epoch in range(num_epochs):\n",
    "            # 前向传播\n",
    "            # print(x_train.shape)\n",
    "            outputs = self.network(x_train)#.reshape(100,)\n",
    "            # print(outputs.shape)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            # print(loss.item())\n",
    "            # 反向传播与优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T04:16:33.073822Z",
     "start_time": "2023-12-01T04:16:32.628121200Z"
    }
   },
   "id": "1c64d011393908f1"
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[241], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m env \u001B[38;5;241m=\u001B[39m make(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmab\u001B[39m\u001B[38;5;124m\"\u001B[39m, debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 10\u001B[0m     \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbay_sub.py\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# env.run([agent, \"neil_and_chris.py\"])\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:267\u001B[0m, in \u001B[0;36mEnvironment.run\u001B[1;34m(self, agents)\u001B[0m\n\u001B[0;32m    265\u001B[0m start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;129;01mand\u001B[39;00m perf_counter() \u001B[38;5;241m-\u001B[39m start \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfiguration\u001B[38;5;241m.\u001B[39mrunTimeout:\n\u001B[1;32m--> 267\u001B[0m     actions, logs \u001B[38;5;241m=\u001B[39m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(actions, logs)\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;129;01mand\u001B[39;00m perf_counter() \u001B[38;5;241m-\u001B[39m start \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfiguration\u001B[38;5;241m.\u001B[39mrunTimeout:\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:697\u001B[0m, in \u001B[0;36mEnvironment.__agent_runner.<locals>.act\u001B[1;34m(none_action)\u001B[0m\n\u001B[0;32m    695\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool\u001B[38;5;241m.\u001B[39mmap(act_agent, act_args)\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 697\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mact_agent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mact_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    699\u001B[0m \u001B[38;5;66;03m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001B[39;00m\n\u001B[0;32m    700\u001B[0m \u001B[38;5;66;03m# This destructures into two lists, a list of actions and a list of logs.\u001B[39;00m\n\u001B[0;32m    701\u001B[0m actions, logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mresults)\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:118\u001B[0m, in \u001B[0;36mact_agent\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m none_action, {}\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mobservation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\agent.py:159\u001B[0m, in \u001B[0;36mAgent.act\u001B[1;34m(self, observation)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    158\u001B[0m     start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[1;32m--> 159\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc(file\u001B[38;5;241m=\u001B[39merr_buffer)\n",
      "Cell \u001B[1;32mIn[240], line 25\u001B[0m, in \u001B[0;36magent\u001B[1;34m(observation, configuration)\u001B[0m\n\u001B[0;32m     22\u001B[0m     feature\u001B[38;5;241m.\u001B[39mfeature_update(step, action, reward, opp_action)\n\u001B[0;32m     23\u001B[0m     total_reward \u001B[38;5;241m=\u001B[39m new_reward\n\u001B[1;32m---> 25\u001B[0m     score \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m100\u001B[39m,)\n\u001B[0;32m     27\u001B[0m     policy_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.03\u001B[39m \u001B[38;5;241m*\u001B[39m score \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.97\u001B[39m \u001B[38;5;241m*\u001B[39m policy_params\n\u001B[0;32m     29\u001B[0m action_probs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(policy_params) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mexp(policy_params))\n",
      "Cell \u001B[1;32mIn[223], line 21\u001B[0m, in \u001B[0;36mSelectActionNet.select_action\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     18\u001B[0m data_feature \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack((step_feature, feature\u001B[38;5;241m.\u001B[39mexpected, feature\u001B[38;5;241m.\u001B[39mpro_lower_bound, feature\u001B[38;5;241m.\u001B[39mpro_upper_bound, feature\u001B[38;5;241m.\u001B[39minterval_length, feature\u001B[38;5;241m.\u001B[39mnum_selection\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m25\u001B[39m, feature\u001B[38;5;241m.\u001B[39mnum_opp_selection\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m25\u001B[39m, feature\u001B[38;5;241m.\u001B[39msum_num\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m50\u001B[39m, feature\u001B[38;5;241m.\u001B[39mnum_ratio, feature\u001B[38;5;241m.\u001B[39mratio_reward, feature\u001B[38;5;241m.\u001B[39mget_reward_th, feature\u001B[38;5;241m.\u001B[39mnum_opp_before\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m10\u001B[39m, feature\u001B[38;5;241m.\u001B[39mnum_before\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m10\u001B[39m, feature\u001B[38;5;241m.\u001B[39mnum_opp_after\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m10\u001B[39m, feature\u001B[38;5;241m.\u001B[39mnum_after\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m10\u001B[39m, feature\u001B[38;5;241m.\u001B[39mopp_bunching, feature\u001B[38;5;241m.\u001B[39mopp_rep_trace), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# print(data_feature.shape)\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m net_expect_value \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_feature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# scores = net_expect_value + interval_score + trace_score + rand\u001B[39;00m\n\u001B[0;32m     24\u001B[0m scores \u001B[38;5;241m=\u001B[39m net_expect_value \u001B[38;5;241m+\u001B[39m interval_score \u001B[38;5;241m+\u001B[39m rand\n",
      "Cell \u001B[1;32mIn[217], line 58\u001B[0m, in \u001B[0;36mNetModel.get_value\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" 输入x应为矩阵形式 \"\"\"\u001B[39;00m\n\u001B[0;32m     57\u001B[0m inputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(x\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m---> 58\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m y \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[217], line 14\u001B[0m, in \u001B[0;36mClasssfyNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 14\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# out = self.relu1(out)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     out \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mtanh(out)\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 经验回放缓冲区以收集样本\n",
    "replay_buffer = ReplayBuffer()\n",
    "# 神经网络模型\n",
    "# network_model = NetModel(17)\n",
    "# 训练\n",
    "for i in range(40):\n",
    "    print(\"epoch\", i)\n",
    "    env = make(\"mab\", debug=True)\n",
    "    if i % 2 == 0:\n",
    "        env.run([agent, \"bay_sub.py\"])\n",
    "        # env.run([agent, \"neil_and_chris.py\"])\n",
    "    elif i % 2 == 1:\n",
    "        env.run([agent, \"Egreedysubmission.py\"])\n",
    "    # if i % 3 == 0:\n",
    "    #     env.run([agent, \"bay_sub.py\"])\n",
    "    # elif i % 3 == 1:\n",
    "    #     env.run([agent, \"Egreedysubmission.py\"])\n",
    "    # elif i % 3 == 2:\n",
    "    #     env.run([agent, \"ucb.py\"])\n",
    "    network_model.train(replay_buffer.buffer)\n",
    "    # network_model.train(replay_buffer.sample(500))\n",
    "    replay_buffer.refresh()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:16.274465400Z",
     "start_time": "2023-12-01T05:58:13.938829900Z"
    }
   },
   "id": "163f7c5565f2fc9b"
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[236], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 测试\u001B[39;00m\n\u001B[0;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m make(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmab\u001B[39m\u001B[38;5;124m\"\u001B[39m, debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbay_sub.py\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmain.py\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m env\u001B[38;5;241m.\u001B[39mrender(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mipython\u001B[39m\u001B[38;5;124m\"\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800\u001B[39m)\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:264\u001B[0m, in \u001B[0;36mEnvironment.run\u001B[1;34m(self, agents)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(agents):\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidArgument(\n\u001B[0;32m    262\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m agents were expected, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(agents)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m was given.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 264\u001B[0m runner \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__agent_runner\u001B[49m\u001B[43m(\u001B[49m\u001B[43magents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    265\u001B[0m start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;129;01mand\u001B[39;00m perf_counter() \u001B[38;5;241m-\u001B[39m start \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfiguration\u001B[38;5;241m.\u001B[39mrunTimeout:\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:670\u001B[0m, in \u001B[0;36mEnvironment.__agent_runner\u001B[1;34m(self, agents)\u001B[0m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__agent_runner\u001B[39m(\u001B[38;5;28mself\u001B[39m, agents):\n\u001B[0;32m    669\u001B[0m     \u001B[38;5;66;03m# Generate the agents.\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m     agents \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    671\u001B[0m         Agent(agent, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    672\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m agent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    673\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    674\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m agents\n\u001B[0;32m    675\u001B[0m     ]\n\u001B[0;32m    677\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(none_action\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    678\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(agents) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate):\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\core.py:671\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__agent_runner\u001B[39m(\u001B[38;5;28mself\u001B[39m, agents):\n\u001B[0;32m    669\u001B[0m     \u001B[38;5;66;03m# Generate the agents.\u001B[39;00m\n\u001B[0;32m    670\u001B[0m     agents \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 671\u001B[0m         \u001B[43mAgent\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    672\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m agent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    673\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    674\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m agents\n\u001B[0;32m    675\u001B[0m     ]\n\u001B[0;32m    677\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(none_action\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    678\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(agents) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate):\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\agent.py:143\u001B[0m, in \u001B[0;36mAgent.__init__\u001B[1;34m(self, raw, environment)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment_name \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mname\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw \u001B[38;5;241m=\u001B[39m raw\n\u001B[1;32m--> 143\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_parallelizable \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuiltin_agents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvironment_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\AI_FALL_23\\Kaggle_PJ\\venv\\lib\\site-packages\\kaggle_environments\\agent.py:116\u001B[0m, in \u001B[0;36mbuild_agent\u001B[1;34m(raw, builtin_agents, environment_name)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(raw):\n\u001B[0;32m    115\u001B[0m     raw_agent \u001B[38;5;241m=\u001B[39m read_file(raw, raw)\n\u001B[1;32m--> 116\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(raw) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m raw \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m raw)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mraw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find : \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m raw)\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# Attempt to execute the last callable or just return the string.\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "env = make(\"mab\", debug=True)\n",
    "env.run([\"bay_sub.py\", \"main.py\"])\n",
    "env.render(mode=\"ipython\", width=800, height=800)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:55:27.157486100Z",
     "start_time": "2023-12-01T05:55:26.879225Z"
    }
   },
   "id": "195376c1a220f5f3"
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: 628 - 651\n",
      "Round 2: 601 - 576\n",
      "Round 3: 664 - 606\n",
      "Round 4: 652 - 595\n",
      "Round 5: 654 - 600\n",
      "Round 6: 663 - 692\n",
      "Round 7: 605 - 592\n",
      "Round 8: 651 - 652\n",
      "Round 9: 684 - 682\n",
      "Round 10: 591 - 613\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "def print_rounds(file1, file2, N=10):\n",
    "    env = make(\"mab\", debug=True)\n",
    "\n",
    "    for i in range(N):\n",
    "        env.run([file1, file2])\n",
    "        p1_score = env.steps[-1][0]['reward']\n",
    "        p2_score = env.steps[-1][1]['reward']\n",
    "        env.reset()\n",
    "        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n",
    "# \"bay_sub.py\"\n",
    "# print_rounds(\"test_softmax_9.py\", agent)\n",
    "print_rounds(\"bay_sub.py\", \"test_softmax_9.py\")\n",
    "# print_rounds(\"ucb.py\", agent)\n",
    "# print_rounds(\"neil_and_chris.py\", agent)\n",
    "# print_rounds(\"Egreedysubmission.py\", agent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T08:59:34.998262400Z",
     "start_time": "2023-12-01T08:58:52.265917500Z"
    }
   },
   "id": "bbe951972b2f62b8"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "torch.save(network_model.network.state_dict(), \"test_1201_4.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:38:05.581309700Z",
     "start_time": "2023-11-30T20:38:05.144425300Z"
    }
   },
   "id": "3822239003e7dae4"
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_model = NetModel(17)\n",
    "state_dict = torch.load(\"test_1201_2.pth\")\n",
    "network_model.network.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T05:58:19.911888700Z",
     "start_time": "2023-12-01T05:58:19.893928100Z"
    }
   },
   "id": "6aa126e4784c330b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7c69aaeef2e250"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_rounds(file1, file2, N=5):\n",
    "    env = make(\"mab\", debug=True)\n",
    "\n",
    "    for i in range(N):\n",
    "        env.run([file1, file2])\n",
    "        p1_score = env.steps[-1][0]['reward']\n",
    "        p2_score = env.steps[-1][1]['reward']\n",
    "        env.reset()\n",
    "        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n",
    "\n",
    "print_rounds(\"ucb.py\", agent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60abb42611e47d1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "def bay_agent(observation, configuration):\n",
    "    global total_reward, bandit, post_a, post_b, feature, selector\n",
    "\n",
    "    n_bandits = configuration.banditCount\n",
    "    player = observation.agentIndex\n",
    "    opponent = 1 - player\n",
    "    step = observation.step\n",
    "    new_reward = observation.reward\n",
    "\n",
    "    if observation.step == 0:\n",
    "        feature = Features(configuration.banditCount)\n",
    "        selector = SelectActionNet(feature)\n",
    "        total_reward = 0\n",
    "        post_a = np.ones(n_bandits)\n",
    "        post_b = np.ones(n_bandits)\n",
    "\n",
    "    else:\n",
    "        r = observation.reward - total_reward\n",
    "        # total_reward = observation.reward\n",
    "\n",
    "        post_a[bandit] += r + (1 - observation.step / 2000)\n",
    "        post_b[bandit] += (1 - r)\n",
    "\n",
    "        action = observation.lastActions[player]\n",
    "        opp_action = observation.lastActions[opponent]\n",
    "        reward = new_reward - total_reward\n",
    "\n",
    "\n",
    "        feature_value = (feature.step/2000, feature.expected[action], feature.pro_lower_bound[action], feature.pro_upper_bound[action],feature.interval_length[action], feature.num_selection[action]/25, feature.num_opp_selection[action]/25, feature.sum_num[action]/50, feature.num_ratio[action], feature.ratio_reward[action], feature.get_reward_th[action], feature.num_opp_before[action]/10, feature.num_before[action]/10, feature.num_opp_after[action]/10, feature.num_after[action]/10, feature.opp_bunching[action], feature.opp_rep_trace[action], reward)\n",
    "        replay_buffer.push(feature_value)\n",
    "\n",
    "        feature.feature_update(step, action, reward, opp_action)\n",
    "        total_reward = new_reward\n",
    "\n",
    "    bound = post_a / (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 3\n",
    "    bandit = int(np.argmax(bound))\n",
    "\n",
    "    return bandit\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231c2c7dd72cf231"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 经验回放缓冲区以收集样本\n",
    "replay_buffer = ReplayBuffer()\n",
    "# 神经网络模型\n",
    "network_model = NetModel(17)\n",
    "# 训练\n",
    "for i in range(100):\n",
    "    print(\"epoch\", i)\n",
    "    env = make(\"mab\", debug=True)\n",
    "    env.run([bay_agent, \"bay_sub.py\"])\n",
    "    network_model.train(replay_buffer.buffer)\n",
    "    replay_buffer.refresh()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9a1cbf115c78a7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试\n",
    "env = make(\"mab\", debug=True)\n",
    "env.run([bay_agent, \"ucb.py\"])\n",
    "env.render(mode=\"ipython\", width=800, height=800)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3985523559339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7efe5bce47c31f13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
